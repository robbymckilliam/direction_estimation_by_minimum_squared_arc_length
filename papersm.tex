%\documentclass[draftcls, onecolumn, 11pt]{../bib/IEEEtran}
\documentclass[journal]{IEEEtran}

\usepackage{mathbf-abbrevs}

\input{defs}

\begin{document}

\title{Direction estimation by minimum squared arc length}

\author{Robby~G.~McKilliam, Barry G. Quinn and I. Vaughan L. Clarkson%
  \thanks{A preliminary version of some of this material is contained in Chapters 5 and 6 of Robby McKilliam's PhD thesis \cite{McKilliam2010thesis}. Robby~McKilliam is with the Institute for Telecommunications Research, The University of South Australia, SA, 5095. Vaughan~Clarkson is with the School of
     Information Technology \& Electrical Engineering, The University
     of Queensland, QLD., 4072, Australia. Barry~Quinn is with the Department of Statistics, Macquarie
     University, Sydney, NSW, 2109, Australia}
     }
% The paper headers 
\markboth{Direction estimation by minimum squared arc length}{DRAFT \today}

% make the title area 
\maketitle

 
\begin{abstract}
Circular statistics has found substantial application in science and engineering. One of the fundamental problems in circular statistics is that of estimating the \emph{mean direction} of a circular random variable from a number of observations. The standard approach in the literature is called the \emph{sample circular mean} and its asymptotic properties are well known. It can also be computed efficiently in a number of arithmetic operations that is linear in the number of observations. In this paper we consider an alternative estimator called the \emph{angular least squares estimator} that is based on minimising squared arc length. We show how this estimator can be computed efficiently in a linear number of operations using an algorithm from algebraic number theory and we derive its asymptotic properties. We show both theoretically and by simulation that in some scenarios the angular least squares estimator is statistically more accurate than the sample circular mean. As such the results in this paper potentially have implications for the wide variety of fields in science, engineering and statistics that currently use the sample circular mean.
\end{abstract}

\begin{IEEEkeywords}
Circular statistics, mean direction estimation, lattice theory, nearest lattice point problem.
\end{IEEEkeywords}


\section{Introduction}\label{sec:introduction}

The field of circular statistics aims to describe the nature of data that is measured in angles or 2-dimensional unit vectors or complex numbers on the unit circle \cite{Mardia_directional_statistics,Jupp_mardia_unified_directional_statistics_1989,mardia_stat_dir_data_jorver_1975,mardia_stat_dir_data_book_1975,Fisher1993,Jammalamadaka_dir_stat_book}.  Such data occur frequently in science, particularly in astronomy, biology \cite{Morellato_circ_stat_phenology_2010,batschelet1981circular,Cochranmigbridcalcircstat2004,Bolesspinylobnavcircstat2003}, medicine \cite{Mann_circstat_med_cement_2003,Lediseasecircstat2003}, geology, geography and meteorology~\cite{CAPACCIONI_imanal_circ_stat_geol_1997,Fullermagfieldrevcircstat1996,Bowersdirstatwindwaves2000,Fisher_bootstrap_geo_1_1990,Marida_geosciences_1981}, and also in engineering, particularly in communications and radar \cite{Fogel1989_bit_synch_zero_crossings,Fogel1988,Elton_circstat_radar_pulse_1994,Elton96robustparameter1996,Clarkson2007,McKilliamLinearTimeBlockPSK2009,McKilliam2007,Clarkson1996}.  A meteorological example is the direction of the wind, and a biological example is the direction of flight taken by a bird. %The field of circular statistics is in some sense an ancient one, probably begun when mankind first started recording the motion of the sun, the moon and the stars.  A thorough historical account of the subject is given by Fisher \cite[Ch.~1]{Fisher1993}.

A fundamental and useful problem in circular statistics is that of estimating the \emph{mean direction} of a circular random variable from a number, say $N$, of observations. For example, if you listen to the weather report you may be told (an estimate of) the direction of the wind.  Obtaining an accurate estimate requires a method for accurately estimating the mean wind direction from a number of observations of the wind direction. The problem of estimating mean direction also appears in signal processing problems such as phase and frequency estimation \cite{Quinn2009_dasp_phase_only_information_loss,Lovell1991,McKilliamFrequencyEstimationByPhaseUnwrapping2009,Quinn2001,Clarkson1999,Tretter1985,McKilliam2010thesis}, polynomial phase estimation~\cite{McKilliam2009asilomar_polyest_lattice,McKilliam2010thesis,Kitchen_polyphase_unwrapping_1994,Morelande_bayes_unwrapping_2009_tsp}, noncoherent detection \cite{McKilliamLinearTimeBlockPSK2009,McKilliam2010thesis,Sweldens2001,Mackenthun1994} and delay and period estimation from sparse observations \cite{Elton96robustparameter1996,Clarkson2007,McKilliam2007,McKilliam2010thesis,Fogel1988,Fogel1989_bit_synch_zero_crossings,Sidiropoulos2005}. 

Unlike the case of random variables on the real line where the mean is uniquely defined, the definition of \emph{mean direction} of a circular random variable is somewhat open to interpretation. The most common definition in the literature, called the \term{circular mean}, is derived by considering circular random variables as unit vectors in 2-dimensions or equivalently complex numbers on the unit circle. Given $N$ observations the appropriate estimator for the circular mean is the \emph{sample circular mean} which can be computed efficiently by averaging $N$ complex numbers. This requires only $O(N)$ arithmetic operations.  The asymptotic properties of the sample circular mean under various assumptions are also well studied \cite{Mardia_directional_statistics,Quinn2009_dasp_phase_only_information_loss,Jammalamadaka_dir_stat_book,McKilliam2010thesis}.

In this paper we consider an alternative definition of mean direction called the \term{unwrapped mean}\index{unwrapped mean} which is based on minimising the expected squared arc length. This definition is similar to that of the so called \emph{circular median} that is based on minimising arc length (without squaring)~\cite{Purkayastha_more_on_circ_median_1995,Liu_ordering_dir_data_1992,Chan_more_median_estimators_on_spheres_1993,mardia_stat_dir_data_jorver_1975,Mardia_directional_statistics,Otieno_phd_circ_2002,Otieno_circ_dir_ecolo_2006}. Surprisingly the \emph{unwrapped mean} has received little attention in the literature. An exception is Chan and He~\cite{Chan_more_median_estimators_on_spheres_1993} who briefly discuss the idea of minimising squared arc length, but focus more intently on minimising the arc length directly.  It appears that estimators based on minimising squared arc length have been overlooked. Perhaps one reason for this is that no efficient way of computing an estimate of the unwrapped mean from a sequence of observations was previously available.  We will describe an estimator called the \emph{angular least squares estimator} that can be computed efficiently using an algorithm from algebraic number theory for finding a \emph{nearest lattice point} in a famous lattice called $A_n^*$ \cite{McKilliam2009CoxeterLattices,McKilliam2008b,McKilliam2008,SPLAG,Conway1982FastQuantDec,Conway1982VoronoiRegions,Martinet2003,Voronoi1908_main_paper}. The number of operations required is $O(N)$, the same as required by the sample circular mean. We also describe the asymptotic properties of the angular least squares estimator showing that it is a strongly consistent estimator of the unwrapped mean and that it is asymptotically normally distributed.

Maximum likelihood estimation has been a common approach to direction estimation in the circular statistics literature (as it is in all of estimation theory)~\cite{Mardia_directional_statistics,Jammalamadaka_dir_stat_book}. Unfortunately, maximum likelihood estimation implicitly assumes that the underlying probability density function is known. For random variables on the real line a common assumption is that of Gaussianity and this is typically justified by appealing to the central limit theorem, but is sometimes also justified experimentally~\cite{Nyquist1924}. For circular random variables there is less evidence to suggest that a particular distribution should be chosen. A common assumption is that of the von Mises distribution~\cite{Upton_approx_cf_vm_1986} or the wrapped normal~\cite{Craig_time_ser_circ_thesis1988}, but such assumptions are not always convincing. This problem has been expressed by many authors and has encouraged the use of non-parametric techniques, such as bootstrapping, for dealing with circular statistics~\cite{RaoJammalamadaka_non_parimetric_bootstrap_circ_1984,Brunner_nonpari_circ_1994,Fisher_bootstrap_geo_1_1990}\cite[Ch.~7]{Jammalamadaka_dir_stat_book}\cite[Ch.~8]{Fisher1993}. 

Unfortunately, these non-parametric techniques are typically computationally intensive and do not suit applications in signal processing such as phase and frequency estimation, polynomial phase estimation, noncoherent detection, or delay estimation that are often implemented on low-power computing devices and operate potentially thousands of times per second.  So our aim is to construct computationally efficient estimators that work well under a wide variety of distributional assumptions. In this paper we show that both the sample circular mean and the angular least squares estimator satisfy this goal.

The paper is organised in the following way.  In Section~\ref{sec:circ-rand-vari} we briefly introduce circular random variables and relevant notation. In Section~\ref{sec:circ-mean-vari} we describe the circular mean and show that the sample circular mean converges almost surely to the circular mean whenever it \emph{exists} (in a sense we will make precise). We show that the sample circular mean is asymptotically normally distributed. These results have appeared previously in the literature in various forms~\cite{Quinn2009_dasp_phase_only_information_loss,Fisher_common_mean_direction_dir_est_no_dist_assumptions1983,Jammalamadaka_dir_stat_book,mardia_stat_dir_data_book_1975}.  In Section~\ref{sec:unwr-mean-vari} we define the unwrapped mean and the angular least squares estimator. It is shown how the angular least squares estimator can be computed in linear-time by finding a nearest point in the lattice $A_n^*$. It is also shown that the angular least squares estimator converges almost surely to the unwrapped mean whenever it \emph{exists} and that it is asymptotically normally distributed. Similar results under stronger conditions were given by Quinn~\cite{Quinn2007}, but the proof we present is more general and takes a different approach. 

The circular mean and the unwrapped mean are not always equal, but in Section~\ref{sec:relationships_circ_unwrapped_mean} we describe a large class of symmetric circular random variables for which they are equal.  For such circular random variables the angular least squares estimator and sample circular mean are estimators of the same quantity and can therefore be compared.  In Section~\ref{sec:comparingestimatorcircmean} we present simulations comparing the performance of the estimators under various distributional conditions.  The central limit theorems derived in Sections~\ref{sec:circ-mean-vari}~and~\ref{sec:unwr-mean-vari} lead to the hypothesis that the angular least squares estimator will be more accurate when the distribution is `light-tailed' whereas the sample circular mean estimator will be more accurate when the distribution is `heavy-tailed'. We have used the terms `light-tailed' and `heavy-tailed' loosely here, but it will become apparent how this is an appropriate and useful rule of thumb. The simulations are in agreement with the theoretical results and also support this `heavy-tail, light-tail' hypothesis.
 

\subsection{Notation}
We write random variables using capital letters, such as $X$ and $Y$ and circular random variables using the capital Greek letters $\Theta$ and $\Phi$.  When describing estimators we use a subscript zero, as in $\mu_0$, to denote the \emph{true} value of a parameter and a hat, as in $\hat{\mu}$, to denote an estimator of $\mu_0$.  We use $\round{x}$ to denote the nearest integer to $x$ with half integers rounded up and use $\fracpart{x} = x - \round{x}$ to denote the \emph{centred} fractional part.


\section{Circular random variables}\label{sec:circ-rand-vari}

As the purpose of circular statistics is to describe the nature of \emph{angles} it is common in the literature to define circular random variables to take values on $[0, 2\pi)$ or $[-\pi, \pi)$. In this paper we find it more convenient to define circular random variables to take values on the interval $[-\nicefrac{1}{2}, \nicefrac{1}{2})$. So, when we refer to an \emph{angle} we mean a real number in the interval $[-\nicefrac{1}{2}, \nicefrac{1}{2})$. This is nonstandard but it will allow us to use notation such as $\round{\cdot}$ for rounding and $\fracpart{\cdot}$ for the fractional part in a convenient way, and will also lead to close ties between circular statistics and number theory or, more specifically, the lattice $A_n^*$. 

It is common in the literature to define a special \emph{circular} probability density function (pdf) $f$ to be periodic with period $1$ (or $2\pi$) so that $f(\theta + k) = f(\theta)$ for any integer $k$ and the integral $\int_{T}f(\theta)d\theta = 1$ where $T$ is any interval of length one.  We will not use this definition.  In this paper a circular random variable is a random variable with range within $[-\nicefrac{1}{2},\nicefrac{1}{2})$. The utility of this is that we have not separated \emph{circular} random variables from \emph{regular} random variables in any way. Sometimes it will be convenient to think of a circular random variable as a random variable that returns values in $[-\nicefrac{1}{2},\nicefrac{1}{2})$ and other times it will be more natural to think of a circular random variable as describing angles wrapped around a circle.  %By not making any severe distinction between circular random variables and regular random variables we are able to switch between these two concepts freely without any notational baggage.  
If we wish to consider the pdf $f$ as a periodic function we shall use $f(\fracpart{x})$, which is clearly periodic with period one for $x \in \reals$ because the fractional part function $\fracpart{x} = x - \round{x}$ has period one.

A circular random variable inherits all the properties of a \emph{regular} random variable.  For example, if $\Theta$ is a circular random variable with pdf $f$ the expected value of a function $g(\Theta)$ of $\Theta$ is given in the usual way by
\[
E[g(\Theta)] = \int_{-\infty}^{\infty}g(\theta)f(\theta)d\theta = \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}g(\theta)f(\theta)d\theta.
\]
This leads to the usual definitions of mean and variance for a circular random variable $E[\Theta]$ and $\var[\Theta] = E[\Theta^2] - E[\Theta]^2$.  A little thought must be given here. The mean $E[\Theta]$ does not necessarily correspond to the \term{mean direction} of $\Theta$ in the sense one might expect.  For example, consider a distribution with equal point masses at $-0.49$ and $0.49$.  The expected value of this random variable is zero, but intuitively a more reasonable estimate for the mean direction is $-\nicefrac{1}{2}$.  One property that any definition of mean direction should have is invariance to translation modulo 1.  That is, if $\Theta$ is a circular random variable with mean direction $\mu$ then for any $\phi \in \reals$ the translated (modulo 1) circular random variable $\fracpart{\Theta + \phi}$ should have mean direction $\fracpart{\mu + \phi}$.  In the following sections we consider two different quantities called the \term{circular mean} and the \term{unwrapped mean} that satisfy this invariance property and correspond with our intuitive notion of mean direction.

%In this paper we will quite often use the mean and variance of a circular random variable i.e. $E[\Theta]$ and $\var[\Theta]$.  When we want one of the other notions, i.e. the circular mean and circular variance, or the unwrapped mean and unwrapped variance we will always state these names in full.  This is in contrast to much of the circular statistics literature that uses the terms mean and variance to refer to the circular mean and circular variance.

%\subsection{Plotting a circular probability density function}
%
%We will consider two ways of plotting the pdf of a circular random variable, one called an \term{unwrapped distribution plot}\index{unwrapped distribution plot} and another called a \term{circular distribution plot}\index{circular distribution plot}.  Both of these plots are displayed in Figure~\ref{fig:plot_initcirc_bimodal}.  On the left is the unwrapped distribution plot.  This is the \emph{usual} way to plot a pdf on the real line.  The value of the pdf is displayed on the vertical axis and the pdf takes nonzero values only on the interval $[-\nicefrac{1}{2}, \nicefrac{1}{2})$.  It is important to remember that $-\nicefrac{1}{2}$ and $\nicefrac{1}{2}$ are \emph{connected} and this notion is lost in the unwrapped distribution plot. This problem is amended by the circular distribution plot displayed on the right.  Here the value of the pdf is given by the distance of the curve from the origin. In both plots the two dotted lines display the minimum and maximum values of the pdf.

\section{The circular mean and its estimation}\label{sec:circ-mean-vari}

Given a circular random variable with pdf $f$ the most common analogue of `mean' and `variance' in the literature is the \term{circular mean} given by 
\[
\mu_{\text{circ}} = \frac{\angle{c}}{2\pi}
\] 
and the \term{circular variance} given by 
\[
\nu = 1 - |c|
\]  
where 
\begin{equation}\label{eq:circmeanint}
c = \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}} e^{2\pi j \theta} f(\theta) d\theta,
\end{equation} 
and $\angle{c}$ and $|c|$ are respectively the complex argument and the magnitude of $c$ \cite[p.~29]{Mardia_directional_statistics}\cite{Fisher1993}. Consider the case when $c = 0$. The circular variance is $1$ but the circular mean is undefined.  In this paper we will say that the distribution \emph{has no circular mean} when $c = 0$. %This is contrast to the mean of a random variable, which is given by $E[\Theta]$ and is always defined.

For example, consider the \term{circular uniform distribution} with pdf displayed in Figure~\ref{fig:circularuniformdist}.  From the symmetry of this distribution it can be seen that the value of $c$ given by the integral~\eqref{eq:circmeanint} will be zero and we therefore conclude that this distribution has no circular mean.  This result conforms well with our intuition. The circular uniform distribution is not the only case where the circular mean is undefined. Consider, for example, the bimodal pdf displayed in Figure~\ref{fig:circnomeanbimodal}. Its symmetry clearly ensures that $c = 0$.

%\begin{figure}[tbp]
%	\centering
%		\includegraphics[height=6cm]{apps_anstar/distplots/initialdistpic.1}
%		\caption{A bimodal circular probability density function. The circular and unwrapped means are marked. They are not equal.}
%		\label{fig:plot_initcirc_bimodal}
%\end{figure}

\begin{figure}[tp]
	\centering 
		\includegraphics[width=\linewidth]{figs/wrappeduniform_var0p08-1.mps}
		\caption{The circular uniform distribution which has no circular mean and no unwrapped mean.}
		\label{fig:circularuniformdist}
\end{figure}

\begin{figure}[tp]
	\centering
		\includegraphics[width=\linewidth]{figs/circnomeansym-1.mps}
		\caption{A bimodal distribution with no circular mean and no unwrapped mean.}
		\label{fig:circnomeanbimodal}
\end{figure}


\subsection{The sample circular mean}\label{sec:sample-circular-mean}

The first estimator we consider is the \emph{sample circular mean}~\cite[p.~15]{Mardia_directional_statistics}\cite{Fisher1993,Jammalamadaka_dir_stat_book}.  Let $\Theta_1, \dots, \Theta_N$ be circular random variables. The sample circular mean is given by
\[
\hat{\mu} = \frac{\angle{\bar{C}}}{2\pi}
\]
where
\[
\bar{C} = \frac{1}{N}\sum_{n=1}^{N}e^{2\pi j \Theta_n}. 
\]  
%It is known that the sample circular mean is the maximum likelihood estimator of the circular mean when $\Theta_1, \dots, \Theta_N$ are independent and identically distributed (i.i.d.) with the \emph{von Mises distribution}~\cite[p.~85]{Mardia_directional_statistics}\cite{Fisher1993}.  
The next theorem describes the asymptotic behaviour of this estimator under some assumptions about the distribution of $\Theta_1, \dots, \Theta_N$.

%Note that if $\Theta$ is a circular random variable then for some real number $\phi$ the random variable given by $\fracpart{\Theta + \phi}$ is a \emph{rotated} version of $\Theta$.  That is, if $f(\theta)$ is the pdf of $\Theta$ then $f(\fracpart{\theta - \phi})$ is the pdf of the circular random variable $\fracpart{\Theta + \phi}$ and if the pdfs are plotted it is seen that $f(\fracpart{\theta - \phi})$ is a \emph{rotated} version of $f(\theta)$.  As an example, consider Figure~\ref{fig:rotatedrandomvar} which displays the pdf of a random variable $\Theta$ and the rotated random variable $\fracpart{\Theta + \nicefrac{1}{4}}$.

% \begin{figure}[tp]
% 	\centering
% 		\includegraphics[width=\linewidth]{figs/rotatedpic-1.mps}
% 		\caption{The pdf of a circular random variable $\Theta$ (left) and the pdf of the \emph{rotated} random variable $\fracpart{\Theta + \tfrac{1}{4}}$.}
% 		\label{fig:rotatedrandomvar}
% \end{figure}


\begin{theorem}\label{thm:asymp_arg_complex_mean}
Let $\Theta_n = \fracpart{\Phi_n + \mu_0}$ where $\Phi_1, \dots, \Phi_N$ are i.i.d. circular random variables with zero circular mean, circular variance $\nu$ and pdf $f$. Let $\hat{\mu}$ denote the sample circular mean of the $\Theta_n$. Then as $N\rightarrow\infty$:
\begin{enumerate}
\item (Strong consistency) $\fracpart{\hat{\mu} - \mu_0}$ converges almost surely to zero.
\item (Asymptotic normality) The distribution of $\sqrt{N}\fracpart{\hat{\mu} - \mu_0}$ converges to the normal with zero mean and variance 
\begin{equation}\label{eq:asympvarscm}
\frac{E[\sin^2(2\pi\Phi_1)]}{4\pi^2(1 - \nu)^2}.
\end{equation}
\end{enumerate}
\end{theorem}
A proof of this theorem is in \cite[p.~88]{McKilliam2010thesis}. Similar proofs in various different forms and under different conditions are given in \cite{Quinn2009_dasp_phase_only_information_loss,Fisher_common_mean_direction_dir_est_no_dist_assumptions1983}~\cite[Chapter~4]{Jammalamadaka_dir_stat_book}~\cite[p.~111]{mardia_stat_dir_data_book_1975}. In the literature the expected value $E[\sin^2(2\pi\Phi_1)]$ is often written in the form
\[
E[\sin^2(2\pi\Phi_1)] = \frac{1}{2} E[ 1 - \cos(4\pi\Phi_1) ] = \frac{1}{2}(1 - m_2)
\]
where $m_2 = E[\cos(4\pi\Phi_1)]$ is the real part of the \emph{second trigonometric moment} of $\Phi_1$ \cite{Fisher_common_mean_direction_dir_est_no_dist_assumptions1983,Fisher1993}. The theorem places conditions on the \emph{fractional part} of the difference, i.e. $\fracpart{\hat{\mu} - \mu_0}$, between the \emph{true} circular mean $\mu_0$ and the estimated circular mean $\hat{\mu}$ rather than directly on the difference $\hat{\mu} - \mu_0$.  This makes sense because the angles $\mu_0$ and $\mu_0 + k$ are equivalent for any integer $k$. So, for example, we expect the angles $0.49$ and $-0.49$ to be \emph{close} together, the difference between them being $\vert\fracpart{-0.49 - 0.49}\vert = 0.02$, and \emph{not} $\vert -0.49 - 0.49\vert = 0.98$. In this way, the theorem asserts that the sample circular mean converges to the circular mean whenever it exists. 

% Confidence intervals can be computed by estimating $E[\sin^2(2\pi\Phi_1)]$ as
% \[
% \frac{1}{N}\sum_{n=1}^N{\sin^2(2\pi\Phi_n)}
% \]
% and estimating the value of the circular variance $\nu$ as 
% \[
% 1 - \abs{\frac{1}{N}\sum_{n=1}^N{e^{2\pi j\Phi_n}}}.
% \] 

%So, this theorem can be regarded as anlogue of the strong law of large numbers and also the central limit theorem but applied to the circular mean.
% \begin{proof}
% From \eqref{eq:direction_est_vectormean} and \eqref{eq:direction_samp_circ_mean} we have
% \[
% \hat{\mu} = \frac{1}{2\pi}\angle\left( N^{-1}\sum_{n=1}^{N}e^{2\pi j \fracpart{\mu_0 + \Phi_n }} \right).
% \]
% Subtracting $\mu_0$ from both sides and taking fractional parts,
% \begin{equation}
% \fracpart{ \hat{\mu} - \mu_0} = \frac{1}{2\pi} \angle\left(   N^{-1}\sum_{n=1}^{N}e^{2\pi j \Phi_n } \right). \label{eq:anglebarc_and_anglesumexpx}
% \end{equation}
% Because the $\Phi_n$ have zero circular mean the expectation $E[e^{2\pi j \Phi_n }] = 1 - \nu$ is a positive real and as $\{\Phi_n\}$ is ergodic we have
% \[
% N^{-1}\sum_{n=1}^{N}e^{2\pi j \Phi_n } \rightarrow E[e^{2\pi j \Phi_n }] = 1 - \nu
% \]
% almost surely as $N$ goes to infinity. As the complex argument $\angle(1 - \nu) = 0$ then $\fracpart{\hat{\mu} - \mu_0} \rightarrow 0$ almost surely as $N \rightarrow \infty$. The completes the proof of strong consistency.

% To prove the central limit theorem let
% \begin{align*}
% \mathcal{I}_N &= \sum_{n=1}^{N}\sin(2\pi\Phi_n) \qquad \text{and} \\
%  \mathcal{R}_N &+ N(1 - \nu) = \sum_{n=1}^{N}\cos(2\pi\Phi_n)
% \end{align*}
% denote the real and imaginary parts of $\sum_{n=1}^{N}e^{2\pi j \Phi_n }$. From Corollary~\ref{cor:circmeancor} we have that $\mathcal{R}_N / N$ and $\mathcal{I}_N / N$ are $O_p(N^{-1/2})$ and $\mathcal{I}_N/\sqrt{N}$ converges to the zero mean normal with variance $h$. So, 
% \begin{align*}
% \sqrt{N}\fracpart{\hat{\mu} - \mu_0} &= \frac{\sqrt{N}}{2\pi}\angle\left(1 - \nu + \mathcal{R}_N/N + j\mathcal{I}_N/N \right) \\
% &= \frac{\sqrt{N}}{2\pi} \left( \frac{\mathcal{I}_N/N}{1 - \nu + \mathcal{R}_N/N}  + O_p(N^{-1}) \right) \\
% &= \frac{\mathcal{I}_N/\sqrt{N}}{2\pi(1 - \nu)} + O_p(N^{-1/2}),
% \end{align*}
% follows by taking a first order approximation of the arctangent function. So $\sqrt{N}\fracpart{\hat{\mu} - \mu_0}$ converges in distribution to the normal with zero mean and variance $\frac{h}{4\pi^2(1 - \nu)^2}$ by Corollary~\ref{cor:circmeancor}.
% \end{proof}

%Note that the theorem holds whenever the circular mean of the $\Theta_n$ exists. A proof in the case where $f$ is symmetric and unimodal is given by \citet{Quinn2009_dasp_phase_only_information_loss}. 
%Computing the asymptotic variance given by the theorem requires calculating $\sigma_s^2$. In general this can be numerically evaluated, but for some circular distributions reasonably simple expressions can be found. It has been shown by \cite{Quinn2009_dasp_phase_only_information_loss} that when $\fracpart{\Theta_n - \mu_0}$ has the $\vonmises(0,\kappa)$ distribution
%\[
%\frac{\sigma_s^2}{(1 - \nu)^2} = \frac{I_0(\kappa)}{\kappa I_1(\kappa)}
%\]
%and when $\fracpart{\Theta_n - \mu_0}$ has the $\projnorm(1, \sigma^2\Ibf)$ distribution
%\[
%\frac{\sigma_s^2}{(1 - \nu)^2} = \frac{8\sigma^4\left(e^{2\nu} - 1\right)}{\pi\left(I_0\left(\nu\right) + I_1\left(\nu \right)  \right)^2}
%\]
%where $\nu = (4\sigma^2)^{-1}$. It is also straightforward to show that when $\fracpart{\Theta_n - \mu_0}$ has the $\wrapunif(0, \sigma^2)$ distribution, where $\sigma^2 < \nicefrac{1}{12}$, then
%\[
%\frac{\sigma_s^2}{(1 - \nu)^2} = \frac{4 r^2 - r\sin(4r)}{2\sin^2(2r) },
%\]
%where $r = \sqrt{3}\pi\sigma$. A closed-form expression for the wrapped normal distribution does not appear to exist and in this case we resort to numerical evaluation.




\section{The unwrapped mean and its estimation}\label{sec:unwr-mean-vari}

Alternatives to the circular mean and circular variance are the \term{unwrapped mean} and \term{unwrapped variance}.  We have chosen the term `unwrapped' because the original motivation for the unwrapped mean and the angular least squares estimator came from the \emph{phase unwrapping} literature, in particular the problems of \emph{frequency estimation} and \emph{polynomial phase estimation} \cite{Tretter1985,McKilliamFrequencyEstimationByPhaseUnwrapping2009,Quinn_on_kay_2000,Morelande_bayes_unwrapping_2009_tsp,Fowler2002_freq_est_by_phases,McKilliam_polyphase_est_icassp_2011}. The idea behind the angular least squares estimator is to `unwrap' the circle as will be seen in the next section. Let $\Theta$ be a circular random variable.  The unwrapped mean of $\Theta$ is the angle $\mu_{\text{unwrap}}$ such that the translated circular random variable $\fracpart{\Theta - \mu_{\text{unwrap}}}$ has minimum squared expectation, that is,
\begin{align}
\mu_{\text{unwrap}}  &= \arg \min_{\mu \in [-\nicefrac{1}{2}, \nicefrac{1}{2})}E\fracpart{\Theta - \mu}^2 \nonumber \\
&= \arg \min_{\mu \in [-\nicefrac{1}{2}, \nicefrac{1}{2})}\int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{\theta - \mu}^2f(\theta) d\theta. \label{eq:minunrappedmeandef}
\end{align}
The unwrapped variance of $\Theta$, denoted $\sigma^2$, is given by 
\[
\sigma^2 = E\fracpart{\Theta - \mu_{\text{unwrap}}}^2 = \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{\theta - \mu_{\text{unwrap}}}^2f(\theta) d\theta.
\]
%Intuitively the unwrapped mean is such that the pdf of $\fracpart{\Theta - \mu_{\text{unwrap}}}$ is mostly centred around zero.

For some circular distributions the minimisation that defines the unwrapped mean~(\ref{eq:minunrappedmeandef}) might not be unique.  For example, consider the bimodal probability density function depicted in Figure~\ref{fig:circnomeanbimodal} for which there would by two minima, one at $\nicefrac{1}{2}$ and one at $-\nicefrac{1}{2}$ (it may at first seem that the minima would be at the modes $\nicefrac{1}{4}$ and $-\nicefrac{1}{4}$, but this is not true in this case).  Another example is the circular uniform distribution (Figure~\ref{fig:circularuniformdist}) where the integral from~\eqref{eq:minunrappedmeandef} is $\nicefrac{1}{12}$ for all $\mu$.  We say that such distributions have no unwrapped mean.

% A circular random variable, $\Theta$, with zero unwrapped mean, i.e. $\mu_{\text{unwrap}} = 0$, has the special property that the unwrapped mean is equal to the mean, that is
% \begin{equation}\label{eq:unwrapmean0=mean}
% \mu_{\text{unwrap}} = E[\Theta] = 0
% \end{equation}
% and the unwrapped variance is equal to the variance, that is
% \begin{equation}\label{eq:unwrapvar0=var}
% \sigma^2 = \var[\Theta].
% \end{equation}

The next lemma describes an interesting property about circular random variables with zero unwrapped mean.  This property will be useful when we consider estimating the unwrapped mean using the angular least squares estimator. 

\begin{lemma}\label{lem:antipolalzerounwpmean}
Let $\Theta$ be a circular random variable with zero unwrapped mean, unwrapped variance $\sigma^2$ and pdf $f$ such that $f(\fracpart{x})$ is continuous at $x = -\nicefrac{1}{2}$.  Then $f(-\nicefrac{1}{2}) \leq 1$.
\end{lemma}
\begin{proof}
Define the function
\[
g(x) = E\fracpart{\Theta - x}^2 = \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{\theta - x}^2f(\theta) \, d\theta.
\]
Because $\Theta$ has zero unwrapped mean, $g$ is \emph{uniquely} minimised at zero with $g(0) = \sigma^2$.  The proof now proceeds by contradiction. Assume that $f(-\nicefrac{1}{2}) > 1$.  Since $f(\fracpart{x})$ is continuous at $-\nicefrac{1}{2}$, there exists a $\delta > 0$ such that $f(\fracpart{\nicefrac{1}{2} + x}) \geq 1$ for all $x \in (-\delta, \delta)$.  Consider $x$ such that $0 < x < \delta$.  Now $g$ evaluated at $-x$ is,
\begin{align*}
&g(-x) = \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{\theta + x}^2 f(\theta) \, d\theta. \\
&= \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2} - x}(\theta + x)^2 f(\theta) \, d\theta + \int_{\nicefrac{1}{2} - x}^{\nicefrac{1}{2}}(\theta + x - 1)^2 f(\theta) \, d\theta \\
&= \sigma^2 + x^2 + \int_{\nicefrac{1}{2} - x}^{\nicefrac{1}{2}}(1 - 2x - 2\theta)f(\theta) \, d\theta.
\end{align*}
For $\theta \in [\nicefrac{1}{2} - x, \nicefrac{1}{2})$, $1 - 2x - 2\theta \leq 0$ and $f(\theta) \geq 1$. Thus
\begin{align*}
g(-x) &\leq \sigma^2 + x^2 + \int_{\nicefrac{1}{2} - x}^{\nicefrac{1}{2}}(1 - 2x - 2\theta) \, d\theta \\
&\leq \sigma^2 + x^2 - x^2 \leq \sigma^2,
\end{align*}
contradicting the fact that $g(0) = \sigma^2$ is the unique minimiser of $g$.
\end{proof}


% \begin{theorem}\label{thm:zerounwpmeanprobbound}
% Let $\Theta$ be a circular random variable with zero unwrapped mean and unwrapped variance $\sigma^2$ and pdf $f$.  If the periodic function $f(\fracpart{x})$ is continuous at $x = \nicefrac{1}{2}$ then the value of the pdf at $-\nicefrac{1}{2}$ less than or equal to one.  That is, $f(-\nicefrac{1}{2}) \leq 1$.
% \end{theorem}
% \begin{proof}
% The proof requires the function
% \[
% g(x) = \var\fracpart{\Theta - x} = \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{\theta - x}^2f(\theta) \, d\theta.
% \]
% Because $\Theta$ has zero unwrapped mean the function $g$ is \emph{uniquely} minimised at zero at which $g(0) = \sigma^2$. %As $\fracpart{\theta - x} = \fracpart{\theta - \fracpart{x}}$ then we acutaully have that $x = 0$ is the unique minimiser of $g$ for $x \in [-1, 1)$.  That is,
% %\[
% %\sigma^2 = \min_{x \in (-1, 1)} g(x)
% %\]
% %and this minimum is \emph{uniquely} attained at $x = 0$.
% The proofs now proceeds by contradiction. We shall show that if the statement is false, then $g$ is not uniquely minimised at zero.

% Assume that statement 1 is false and $f(-\nicefrac{1}{2}) > 1$. Due to the continuity of $f(\fracpart{x})$ at $-\nicefrac{1}{2}$ there exists a $\delta > 0$ such that $f(\fracpart{\nicefrac{1}{2} + x}) \geq 1$ for all $x \in (-\delta, \delta)$. Consider $x$ such that $0 < x < \delta$, then $g$ evaluated at $-x$ is,
% \begin{align*}
% &g(-x) = \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{\theta + x}^2 f(\theta) \, d\theta. \\
% &= \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2} - x}(\theta + x)^2 f(\theta) \, d\theta + \int_{\nicefrac{1}{2} - x}^{\nicefrac{1}{2}}(\theta + x - 1)^2 f(\theta) \, d\theta \\
% &= \sigma^2 + x^2 + \int_{\nicefrac{1}{2} - x}^{\nicefrac{1}{2}}(1 - 2x - 2\theta)f(\theta) \, d\theta.
% \end{align*}
% For $\theta \in [\nicefrac{1}{2} - x, \nicefrac{1}{2})$ the value of $1 - 2x - 2\theta \leq 0$ and also $f(\theta) \geq 1$ so the integral in the last line of the above equation can be bounded above by setting $f(\theta) = 1$ giving
% \begin{align*}
% g(-x) &\leq \sigma^2 + x^2 + \int_{\nicefrac{1}{2} - x}^{\nicefrac{1}{2}}(1 - 2x - 2\theta) \, d\theta \\
% &\leq \sigma^2 + x^2 - x^2 \leq \sigma^2.
% \end{align*}
% This violates that $g(0) = \sigma^2$ is the unique minimiser of $g$. So statement 1 is true.
% \end{proof}



\subsection{The angular least squares estimator}\label{sec:angul-least-squar}

We now describe the angular least squares estimator of the unwrapped mean.  Let $\Theta_1,\dots,\Theta_N$ be circular random variables and let
\[
SS(\mu) = \sum_{n=1}^{N}{ \fracpart{\Theta_n - \mu}^2 }.
\]
The angular least squares estimator is the minimiser of $SS(\mu)$. 
%Intuitively this estimator chooses the angle $\hat{\mu}$ such that the rotated random variables $\fracpart{\Theta_n - \hat{\mu}}$ are closet to zero in the sense of square arc length. 
This should be compared with the \emph{circular median}, which is defined as the minimiser of
\begin{equation}\label{eq:circmedobjfunc}
\sum_{n=1}^{N}{ \abs{\fracpart{\Theta_n - \mu}} }.
\end{equation}
The circular median has been studied quite thoroughly in the literature \cite{Purkayastha_more_on_circ_median_1995,Fisher_spherical_medians_1985,mardia_stat_dir_data_book_1975}, but the sum of squares function $SS$ has received comparatively little attention. One reason for this might be that~\eqref{eq:circmedobjfunc} is convex in $\mu$ and therefore computing the circular median is straightforward~\cite{Fisher_spherical_medians_1985}\footnote{In~\cite{Fisher_spherical_medians_1985} Fisher actually deals with computing the circular median in more dimensions that two, i.e. the \emph{spherical median}. Due to convexity computing the median even in these higher dimensions can be achieved by standard optimisation techniques~\cite[p.~347]{Fisher_spherical_medians_1985}.}. On the other hand $SS$ is not convex in $\mu$ and therefore minimising $SS$ is potentially difficult. This fact may have prompted authors to ignore the angular least squares estimator.  

However, we shall show that the angular least squares estimator can be computed quickly in $O(N)$ operations by finding a nearest point in the lattice $A_{N-1}^*$ \cite{McKilliam2008,McKilliam2009CoxeterLattices}~\cite[p.~108]{SPLAG}.  To see this write the sum of squares function $SS$ as
\[
SS(\mu) = \sum_{n=1}^{N}{ \left( \Theta_n - \mu - W_n \right)^2 }
\]
where $W_n$ are integers given by $\round{\Theta_n - \mu}$. The $W_n$ are called \term{wrapping variables}\index{wrapping variables} because they describe how the $\Theta_n - \mu$ \emph{wrap} around the circle.  By considering the $W_n$ as nuisance parameters to be estimated $SS$ can be written as a function of both $\mu$ and the $W_n$ and the angular least squares estimator can be found by minimising over both $\mu$ and the $W_n$. It may at first appear that we have made this problem hard for ourselves because there was only one variable, $\mu$, to minimise over and now there are $N+1$ variables, $\mu$ and all of the $W_1, \dots, W_N$.  However, we will show how this joint minimisation problem can be solved efficiently.  Write $SS$ as a function of $\mu$ and the $W_n$ using vectors\footnote{We have slightly abused notation here by reusing $SS$. This should not cause any confusion as $SS(\mu)$ and $SS(\mu,\wbf)$ and $SS(\wbf)$ are easily told apart by their inputs.} as
\[
SS(\mu, \wbf) = \| \thetabf - \mu\onebf - \wbf \|^2
\]
where $\thetabf = [\Theta_1, \dots, \Theta_N]^\prime$ and $\wbf = [W_1, \dots, W_N]^\prime$ and $\onebf$ is the all ones column vector, $^\prime$ denotes the vector transpose and $\|\xbf\|^2 = \xbf^\prime\xbf$ is the Euclidean norm. Fixing $\wbf$ and minimising with respect to $\mu$ gives
\begin{equation} \label{eq:hat_theta_cal_anstar}
\hat{\mu} = \frac{\dotprod{(\thetabf - \wbf)}{\onebf}}{\dotprod{\onebf}{\onebf}}.
\end{equation}
Substituting this into $SS(\mu, \wbf)$ gives
\[
SS(\wbf) = \min_{\mu}SS(\mu, \wbf) = \| \Qbf\thetabf - \Qbf\wbf \|^2
\]
where $\Qbf = \Ibf - \onebf\onebf^\prime / (\onebf^\prime\onebf)$ and $\Ibf$ is the $N \times N$ identity matrix. $\Qbf$ is related to the generator matrix for the lattice $A_{N-1}^*$~\cite{McKilliam2008,McKilliam2009CoxeterLattices}~\cite[p. 115]{SPLAG}. It follows  that $\Qbf\wbf$ is a lattice point in $A_{N-1}^*$ and that minimising $SS(\wbf)$ is equivalent to finding the nearest lattice point in $A_{N-1}^*$ to $\Qbf\thetabf$.  A number of algorithms exist to compute the nearest point, the fastest of which require a linear number of operations in $N$~\cite{McKilliam2009CoxeterLattices,McKilliam2008,McKilliam2008b}. Once $\hat{\wbf}$ has been found, the estimate $\hat{\mu}$ is given by substituting $\hat{\wbf}$ for $\wbf$ in~\eqref{eq:hat_theta_cal_anstar}.  Because the nearest point can be found in $O(N)$ operations the angular least squares estimator can be computed in linear-time. The next theorem describes the asymptotic behaviour of the estimator.

\begin{theorem}\label{thm:asymp_proof_m=0}
Let $\Theta_n = \fracpart{\Phi_n + \mu_0}$ where $\Phi_1, \dots, \Phi_N$ are i.i.d. circular random variables with zero unwrapped mean, unwrapped variance $\sigma^2$ and pdf $f$. Let $\hat{\mu}$ denote the angular least squares estimator of the $\Theta_n$.  Then as $N\rightarrow\infty$:
\begin{enumerate}
\item (Strong consistency) $\fracpart{\hat{\mu} - \mu_0}$ converges to zero almost surely. 
\item (Asymptotic normality) If $f(\fracpart{x})$ is continuous at $x = -\nicefrac{1}{2}$ and $f(-\nicefrac{1}{2}) \leq 1$ then the distribution of $\sqrt{N}\fracpart{\hat{\mu} - \mu_0}$ converges to the normal with zero mean and variance
\begin{equation}\label{eq:asympvarm=0}
\frac{\sigma^2}{\big(1 - f(-\nicefrac{1}{2}) \big)^2}.
\end{equation}
\end{enumerate}
\end{theorem}
%This theorem asserts that the angular least squares estimator converges to the unwrapped mean whenever it exists (again the fractional part of the difference $\fracpart{\hat{\mu} - \mu_0}$ is used, as is natural). 
Before we begin the proof we shall comment on the requirements for part 2 of this theorem.  From Lemma \ref{lem:antipolalzerounwpmean} we know that $f(-\nicefrac{1}{2}) \leq 1$, so the only case not handled by this theorem is when equality holds, i.e. when $f(-\nicefrac{1}{2}) = 1$ or when $f(\fracpart{x})$ is not continuous at $x = -\nicefrac{1}{2}$. In these exceptional cases other expressions for the asymptotic variance can be found, but this comes at a substantial increase in complexity, so we have opted to omit them. Circular distributions that do not satisfy these requirements are unlikely to be needed in practice.

We now prove Theorem~\ref{thm:asymp_proof_m=0}. A similar result under stronger conditions is given by Quinn~\cite{Quinn2007}, but the proof we present here is more general and takes a different approach. The proof follows the format of more general results for frequency estimation by \emph{phase unwrapping} \cite{McKilliamFrequencyEstimationByPhaseUnwrapping2009} and for polynomial phase estimation by phase unwrapping \cite[Ch.~8]{McKilliam2010thesis}. In this special case the proof can be (and has been) substantially simplified.
\begin{proof}
Substituting $\Theta_n = \fracpart{\Phi_n + \mu_0}$ into $SS$ gives
\begin{align*}
SS(\mu) &= \sum_{n=1}^{N}{ \fracpart{\fracpart{\Phi_n + \mu_0} - \mu}^2 } \\
&= \sum_{n=1}^{N}{ \fracpart{\Phi_n - \fracpart{\mu - \mu_0}}^2 } \\
&= \sum_{n=1}^{N}{ \fracpart{\Phi_n - \lambda}^2 } = NL_N(\lambda),
\end{align*}
where $\lambda = \fracpart{\mu - \mu_0}$ and $L_N(\lambda) = \tfrac{1}{N}SS(\mu)$. Let $\hat{\lambda}$ be the minimiser of $L_N$, i.e. $\hat{\lambda} = \fracpart{\hat{\mu} - \mu_0}$. We will show that the minimiser of $L_N$ converges to zero almost surely as $N \rightarrow \infty$. To do this we first show that $L_N$ converges almost surely and uniformly in $\lambda \in [-\nicefrac{1}{2}, \nicefrac{1}{2})$ to its expectation. That is,
\begin{equation}\label{eq:supELNLN}
\sup_{\lambda \in [-\nicefrac{1}{2}, \nicefrac{1}{2})}| L_N(\lambda) - EL_N(\lambda) | \rightarrow 0
\end{equation}
almost surely as $N \rightarrow \infty$. A proof is given in Lemma~\ref{lem:limsupSandES} of the appendix. %We can now reason about the minimiser of $L_N$ using the minimiser of $EL_N$. 
Because $\hat{\lambda}$ minimises $L_N$,
\[
0 \leq L_N(0) - L_N(\hat{\lambda}).
\] 
Let
\[
Q(\lambda) = EL_N(\lambda) = \frac{1}{N}\sum_{n=1}^{N}E\fracpart{\Phi_n - \lambda}^2 = E\fracpart{\Phi_1 - \lambda}^2
\] 
and observe that $Q(\lambda)$ is continuous and is uniquely minimised when $\lambda = 0$ because $\Phi_1$ has zero unwrapped mean. Therefore
\[
0 \leq Q(\hat{\lambda}) - Q(0).
\]
Adding both inequalities and taking appropriate absolute values gives
\begin{align*}
0 &\leq Q(\hat{\lambda}) - Q(0) \\ 
& \leq Q(\hat{\lambda}) - Q(0) + L_N(0) - L_N(\hat{\lambda})\\
& \leq |Q(\hat{\lambda}) - L_N(\hat{\lambda})| + |L_N(0) - Q(0)|.
\end{align*}
and because of~\eqref{eq:supELNLN} the last line converges almost surely to zero as $N \rightarrow \infty$ and therefore $Q(\hat{\lambda}) \rightarrow Q(0)$ almost surely. To complete the proof of strong consistency we use the device of Jenrich~\cite{Jenrich_asymp_nonlin_ulln_1969}. We use the
subscript $N$ to emphasise that $\hat{\lambda}$ depends on $N.$ Suppose that the sequence
$\left\{ \hat{\lambda}_{N}\right\} $ does not converge almost surely to $0$ as $N\rightarrow\infty,$ and let $A$ be the subset of the sample space on which $\left\{  \hat{\lambda}_{N}\right\}  $ does not converge to 0 as $N\rightarrow\infty$. Then $P\left(  A\right)  >0$.  For any point in $A$, there is an infinite subsequence $\left\{  \hat{\lambda}_{N_{j}}\right\}$ which converges to some $\lambda^{\prime}\neq0$ in the compact interval $[-\nicefrac{1}{2}, \nicefrac{1}{2}]$.  Thus $Q\left(\hat{\lambda}_{N_{j}}\right)$ converges to $Q\left(\lambda^{\prime}\right)$, since $Q$ is continuous in $\lambda$.  However, $Q\left(\lambda^{\prime}\right) > Q\left(  0\right)$, contradicting the fact that $Q\left( \hat{\lambda}_{N_{j}}\right)  $ converges to $Q(0)$.  It follows that $\left\{\hat{\lambda}_{N}\right\}$ converges almost
surely to $0$ as $N\rightarrow\infty$.

We now prove asymptotic normality. Observe from \eqref{eq:hat_theta_cal_anstar} that
\[
\hat{\mu} = \frac{1}{N}\sum_{n=1}^{N}{\left( \Theta_n - \hat{W}_n \right)}
\]
where $\hat{W}_n = \round{\Theta_n - \hat{\mu}}$ are the \emph{estimated} wrapping variables. Substituting $\Theta_n = \fracpart{\Phi_n + \mu_0}$ gives
\begin{align*}
\hat{\mu} &= %\frac{1}{N}\sum_{n=1}^{N} \fracpart{\Phi_n + \mu_0} - \hat{W}_n  \\
\frac{1}{N}\sum_{n=1}^{N} \left( \fracpart{\Phi_n + \mu_0} - \round{ \fracpart{\Phi_n + \mu_0} - \hat{\mu}} \right) \\
&= \frac{1}{N}\sum_{n=1}^{N} \left( \Phi_n + \mu_0 - \round{ \Phi_n + \mu_0 - \hat{\mu}} \right)
\end{align*}
where, to obtain the last line, we have used the fact that $\round{\fracpart{x} + y} = \round{x + y} - \round{x}$. Subtracting $\mu_0 + \round{\hat{\mu} - \mu_0}$ from both sides gives
\[
\hat{\mu} - \mu_0 - \round{\hat{\mu} - \mu_0} = \frac{1}{N}\sum_{n=1}^{N} \left( \Phi_n - \round{ \Phi_n - \fracpart{\hat{\mu} - \mu_0}} \right),
\]
or
\[
\hat{\lambda} = \frac{1}{N}\sum_{n=1}^{N} \left( \Phi_n - \round{ \Phi_n - \hat{\lambda} }\right).
\]

We require an expression for the distribution of
%\begin{equation}\label{eq:sqrtNlambda}
\[
\sqrt{N}\hat{\lambda} = \frac{1}{\sqrt{N}}\sum_{n=1}^{N} \left( \Phi_n - \round{ \Phi_n - \hat{\lambda} }\right).
\]
%\end{equation} 
It is proved in Lemma~\ref{lem:EI_n} in the appendix that
\[
\frac{1}{\sqrt{N}}\sum_{n=1}^{N} \round{ \Phi_n - \hat{\lambda}} = o_P(1)  - \sqrt{N} \hat{\lambda}( f(-\nicefrac{1}{2}) + o_p(1) )
\]
where $o_p(1)$ converges in probability to zero as $N\rightarrow\infty$.  So
\[
\sqrt{N}\hat{\lambda}(1 - f(-\nicefrac{1}{2}) + o_p(1) ) = o_P(1) + \frac{1}{\sqrt{N}}\sum_{n=1}^{N} \Phi_n
\]
and therefore $\sqrt{N}\hat{\lambda} = \sqrt{N}\fracpart{\hat{\mu} - \mu_0}$ converges in probability to
\[
\frac{\frac{1}{\sqrt{N}}\sum_{n=1}^{N} \Phi_n}{1 - f(-\nicefrac{1}{2})}.
\]
The $\Phi_1\dots,\Phi_N$ are i.i.d. with zero unwrapped mean and unwrapped variance $\sigma^2$ so $E[\Phi_n] = 0$ and $\var[\Phi_n] = \sigma^2$ and, by the standard central limit theorem, the distribution of $\sqrt{N}\fracpart{\hat{\mu} - \mu_0}$ converges to the zero mean normal with variance $\sigma^2(1 - f(-\nicefrac{1}{2}))^{-2}$ as required. 
\end{proof}



\section{Relationships between the circular and unwrapped mean}\label{sec:relationships_circ_unwrapped_mean}

In general the unwrapped mean and the circular mean are different. In fact it is reasonably easy to construct distributions which \emph{have} a circular mean but \emph{do not} have an unwrapped mean and other distributions which \emph{have} an unwrapped mean but \emph{do not} have a circular mean. As a simple example consider the circular random variable that takes the value 0 with probability 0.6 and the value $-\nicefrac{1}{2}$ with probability 0.4.  The circular mean is clearly 0 but the unwrapped mean is not defined because (\ref{eq:minunrappedmeandef}) has two minima, one at 0.2 and the other at -0.2 both of value 0.06.  However, for some distributions both means are defined and they are also equal. It is potentially feasible to classify all such distributions, but this is not attempted here.  We do, however, find a particular class of useful circular distributions that do have equal circular and unwrapped means. These are described in the next theorem.

\begin{theorem}\label{thm:unimean}
Let $\Theta$ be a circular random variable with piecewise continuous pdf $f(\theta)$ that attains its maximum at $0$ with $f(0) > 1$ and $f$ even and nondecreasing on $[-\nicefrac{1}{2}, 0]$. Then $\Theta$ has circular and unwrapped mean equal to zero.\end{theorem}
We omit the proof which is given in \cite[p.~74]{McKilliam2010thesis}. In a sense this result is `intuitively obvious' as the requirements force the distribution to be \emph{concentrated} in a symmetric manner.  Because $f$ is even and nondecreasing on $[-\nicefrac{1}{2}, 0]$ then it is also non increasing on $[0, \nicefrac{1}{2})$. %Combining this with the fact that $f(0) > 1$ immediately implies that the antipodal point $f(-\nicefrac{1}{2}) < 1$ otherwise the integral $\int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}f(\theta)d\theta$ would not equal one. 
The requirements automatically include distributions that are unimodal and symmetric, but they also include distributions with \emph{flat} pieces, for example, many cases of the \term{wrapped uniform distribution} \cite{Mardia_directional_statistics,Fisher1993}~\cite[Sec. 5.5]{McKilliam2010thesis}. However, the theorem \emph{does not} include the circular uniform distribution (Figure~\ref{fig:circularuniformdist}) because the circular uniform distribution has $f(0)$ equal to one but not greater than one.

% \begin{proof}
% Statement (2) follows directly from statement (1) because if $\Theta$ has circular mean $\mu_{\text{circ}}$ and unwrapped mean $\mu_{\text{unwrap}}$ then the rotated circular random variable $\fracpart{\Theta + \phi}$ has circular mean $\fracpart{\mu_{\text{circ}} + \phi}$ and unwrapped mean $\fracpart{\mu_{\text{unwrap}} + \phi}$. It remains to prove statement (1). 

% We first prove that the circular mean $\mu_{\text{circ}} = \tfrac{\angle{c}}{2\pi}$ is zero. It suffices to show that the integral
% \[
% c = \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}} e^{2\pi i \theta} f(\theta) d\theta
% \]
% is a positive real number. Breaking the integral into real and imaginary parts gives
% \[
% c = \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}} \cos(2\pi\theta) f(\theta) d\theta + j\int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}} \sin(2\pi\theta) f(\theta) d\theta.
% \]
% Because $f$ is even and $\sin(\cdot)$ is odd then the imaginary part of $c$ is zero. Also, because $f$ is nondecreasing of $[-\nicefrac{1}{2}, 0]$ and $f(-\nicefrac{1}{2}) < 1$ the integral
% \[
% \int_{-\nicefrac{1}{2}}^{0} \cos(2\pi\theta) f(\theta) d\theta > 0
% \]
% because $\cos(\cdot)$ is increasing and odd about $-\nicefrac{1}{4}$ on the interval $[-\nicefrac{1}{2}, 0]$. As $\cos(\cdot)$ and $f$ are even $c$ is equal to twice the integral above and therefore $c > 0$ and $\angle{c} = 0$ and the circular mean is equal to zero.
% \[
% \int_{-\nicefrac{1}{2}}^{0} \cos(2\pi\theta) f(\theta) d\theta = \int_{0}^{\nicefrac{1}{2}} \cos(2\pi\theta) f(\theta) d\theta
% \]

% We will now prove that the unwrapped mean is also zero. The approach we take is similar to Lemma 1 from~\cite{Quinn2007} and also Lemma 3 from~\cite{McKilliamFrequencyEstimationByPhaseUnwrapping2009}. We need to show that the integral
% \[
% g(\mu) = \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{\theta - \mu}^2f(\theta) d\theta
% \]
% is uniquely minimised at $\mu = 0$. First note that because $f$ is even and $\fracpart{-x} = -\fracpart{x}$, i.e. the fractional part function is odd, we have
% \begin{align*}
% g(-\mu) &= \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{\theta + \mu}^2f(\theta) d\theta \\
% &= \int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}}\fracpart{-\theta + \mu}^2f(\theta) d\theta = g(\mu)
% \end{align*}
% and therefore $g$ is even. Now, if $\mu \geq 0$ then 
% \[
% \fracpart{\theta - \mu} =
% \begin{cases} 
%  \theta - \mu,  & \mbox{if }\theta > -\nicefrac{1}{2} + \mu \\
%  \theta - \mu + 1, & \mbox{if }\theta < -\nicefrac{1}{2} + \mu
% \end{cases}
% \]
% and $g$ is given by
% \begin{align*}
% &g(\mu) = g(-\mu) = \\
% &\int_{-\nicefrac{1}{2} + \mu}^{\nicefrac{1}{2}} (\theta - \mu)^2 f(\theta)d\theta  + \int_{-\nicefrac{1}{2}}^{-\nicefrac{1}{2} + \mu}(\theta - \mu + 1)^2 f(\theta)d\theta \\
% &= g(0) + \mu^2 + \int_{-\nicefrac{1}{2}}^{-\nicefrac{1}{2} + \mu}(1 - 2\mu + 2\theta) f(\theta)d\theta
% \end{align*}
% as the integral $\int_{-\nicefrac{1}{2}}^{\nicefrac{1}{2}} \theta f(\theta)d\theta$ is equal to zero because $f$ is even. Now, when $\mu > 0$, differentiating $g$ with respect to $\mu$ gives
% \begin{align}
% g^{\prime}(\mu) &= 2\mu - 2\int_{-\nicefrac{1}{2}}^{-\nicefrac{1}{2} + \mu}f(\theta)d\theta \label{eq:gdisusslater}\\
% &= 2\mu - 2F( \mu-\nicefrac{1}{2}) \nonumber
% \end{align}
% where $F$ is the cumulative distribution function\index{cumulative distribution function (cdf)} (cdf) of $\Theta$. Since $f$ is even and non-decreasing on $[-\nicefrac{1}{2}, 0)$, $F(-\nicefrac{1}{2}) = 0$, $F(0) = \nicefrac{1}{2}$ and $F(\mu - \nicefrac{1}{2})$
% is convex on $[0, \nicefrac{1}{2})$. Also, because $f(-\nicefrac{1}{2}) < 1$ then $F$ is \emph{strictly} convex on $[0, \nicefrac{1}{2})$. So $g$ is monotonically increasing on $[0, \nicefrac{1}{2})$ and, being even, is monotonically decreasing on $[-\nicefrac{1}{2}, 0)$.  Therefore $g$ is uniquely minimised at zero and the unwrapped mean is equal to zero.
% \end{proof}

% \subsection{Projected circular distributions}\label{sec:projecteddists}
% A common way to construct a circular random variable is to take a complex random variable and \emph{project} it onto the unit circle. These are called \term{projected circular distributions}\index{projected circular distribution}~\cite{Mardia_directional_statistics}. If $X$ is a complex random variable with pdf $f_{\complex}$ then the corresponding projected circular random variable $\Theta$ is given by the complex argument of $X$ divided by $2\pi$, that is 
% \[
% \Theta = \tfrac{1}{2\pi}\angle X.
% \]
% The pdf of $\Theta$ is given by
% \[
% f(\theta) = \int_{0}^{\infty}{ r f_{\complex}\left( r e^{2\pi i \theta} \right) dr }.
% \]

% These distributions arise frequently in engineering problems, for example, phase and frequency estimation~\cite{McKilliamFrequencyEstimationByPhaseUnwrapping2009,Quinn2009_dasp_phase_only_information_loss,Tretter1985,Quinn2007} and polynomial phase estimation~\cite{McKilliam2009asilomar_polyest_lattice,Kitchen_polyphase_unwrapping_1994,Slocumb_polynomial_1994}.

% \begin{theorem}\label{thm:projsymandunimean}
% Let $X$ be the complex random variable given by
% \[
% X = 1 + Z e^{2\pi j \Phi}
% \]
% where $Z$ and $\Phi$ are independent random variables. Let $Z$ have pdf $f_{Z}(z)$ with support on the positive reals such that $z^{-1}f_{Z}(z)$ is non increasing and continuous and differentiable in $z$. Let $\Phi$ be uniformly distributed on $[-\nicefrac{1}{2}, \nicefrac{1}{2})$, i.e. $\Phi$ has the circular uniform distribution and let $\Theta$ be the projected circular random variable
% \[
% \Theta = \frac{1}{2\pi}\angle{X}.
% \]
% Then $\Theta$ is symmetrically distributed about $0$, and unimodal with mode at $0$, and $\Theta$ is unimean with circular and unwrapped means equal to $0$. 
% \end{theorem}
% Before we begin the proof note that the requirement for $z^{-1}f_{Z}(z)$ to be non increasing implies that the probability density function of $Z e^{2\pi j \Phi}$ decreases as we move away from the origin. That is, the pdf of $Z e^{2\pi j \Phi}$ in rectangular coordinates is given by $z^{-1}f_{Z}(z)$, where $z = \sqrt{x^2 + y^2}$ and $x$ and $y$ denotes the real and imaginary parts of $Z e^{2\pi j \Phi}$, and this pdf is non increasing with $z$.  For example, the zero mean complex Gaussian distribution with independent real and imaginary parts satisfies this requirement. 
% \begin{proof}
% The fact that $\Theta$ is unimean follows directly from Theorem~\ref{thm:unimean} after we show that $\Theta$ is symmetric and unimodal. The probability density function of $\Theta$ can be shown to be
% \[
% f(\theta) = \int_0^\infty{ \frac{r f_{Z}\left(\sqrt{r^2 - 2r\cos{2\pi\theta} + 1}\right)}{\sqrt{r^2 - 2r\cos{2\pi \theta} + 1}} dr}.
% \]
% Note that $f$ is continuous because $f_{Z}$ is continuous. Also $f$ is even because $\cos(\cdot)$ is even and therefore $f$ is symmetric about zero, so it only remains to show that $f$ is unimodal with mode at zero.

% Let $a = \cos{2\pi\theta}$, so $a \in [-1, 1]$ and $\theta = 0$ when $a = 1$ and as $a$ decreases from $1$ to $-1$ the magnitude $|\theta|$ increases. Let $z = \sqrt{r^2 - 2ra + 1}$ and note that $z \geq 0$ with equality only when $r = 1$ and $a = \pm 1$ or $a = 0$. The term inside the integral asymptotes when $z$ is equal zero so is not differentiable at these points. For now assume that $a$ is not $0$ or $\pm 1$ to avoid these asymptotes.  Differentiating $f$ with respect to $a$ we obtain
% \[
% \frac{d}{da}f = \int_{0}^{\infty} -\frac{r^2}{z} \frac{d}{dz}\left(\frac{f_{Z}(z)}{z}\right) dr.
% \]
% Now because $z^{-1}f_{Z}(z)$ is non increasing in $z$ and because $z$ and $r$ are positive the term inside the integral above is always positive.  Therefore the integral is positive and $f$ is increasing with $a$. The magnitude $|\theta|$ increases as $a$ decreases so $z$ is decreasing with $|\theta|$. It remains to show that no jump discontinuities occur in $z$ when $a = \pm 1$ or $a = 0$, i.e. when $\theta = \pm \nicefrac{1}{2}$ for $\theta = 0$, but this is trivially the case due to the continuity of $f$. Therefore, as $f(\theta)$ 
%is decreasing with $|\theta|$ and $f(\theta)$ is continuous, we see that $f(\theta)$ is unimodal with mode at $\theta = 0$.
%\end{proof}

\section{Comparing the two estimators}\label{sec:comparingestimatorcircmean}

In this section we compare the angular least squares estimator and the sample circular mean.  %It is important to realise that, in general, these are estimators of different quantities.  Angular least squares estimates the unwrapped mean, while the sample circular mean estimates the circular mean. For circular distributions that have different circular and unwrapped means, this would make a comparison somewhat meaningless.  However, comparisons can be made for distributions with equal circular and unwrapped means, such as those described in Theorem~\ref{thm:unimean}. 
The distributions that we will consider are the von Mises, wrapped normal and wrapped uniform distributions \cite[Ch.~3]{Fisher1993}\cite[Ch.~3]{Mardia_directional_statistics}\cite[Ch.~4]{McKilliam2010thesis} and also a distribution created from a weighted sum of the the von Mises and the circular uniform distributions.  All of these distributions satisfy the requirements of Theorem~\ref{thm:unimean} and therefore have equal circular and unwrapped means.  %As a general rule of thumb we find that the sample circular mean is slightly more accurate when the distribution is `von Mises-like' and the angular least squares estimator is more accurate when the distribution is `uniform-like'.
Figures~\ref{fig:directionest_VonMises} to~\ref{fig:directionest_sumdist} display the sample mean square error (MSE) of the estimators when the number of observations is $N=4,64$ and $1024$. The quantity displayed on the horizontal axis is the unwrapped variance of the circular random variable being estimated. For each value of unwrapped variance $T = 5000$ trials were run to obtain $T$ separate estimates $\hat{\mu}_1, \dots, \hat{\mu}_{T}$. The sample MSE is computed by averaging the squared fractional parts according to $T^{-1}\sum_{t=1}^{T}\fracpart{ \hat{\mu}_t - \mu_0}^2$.

Figure~\ref{fig:directionest_VonMises} displays the MSE when the observations are sampled from the von Mises distribution. The sample circular mean performs slightly better in this case.  Figure~\ref{fig:directionest_Uniform} displays the MSE when the observations are sampled from the wrapped uniform distribution.  For this distribution the angular least squares estimator is more accurate.  Figure~\ref{fig:directionest_normal} shows the MSE when the observations are sampled from the wrapped normal distribution.  In this case, the estimators perform quite similarly.  The sample circular mean is slightly more accurate when the unwrapped variance is larger, i.e. above approximately 0.05, and the angular least square estimator is slightly more accurate when the unwrapped variance is smaller, i.e. less than approximately 0.05.  When the unwrapped variance is sufficiently small both estimators display similar performance.  All figures display the asymptotic variances predicted by Theorems~\ref{thm:asymp_arg_complex_mean} and~\ref{thm:asymp_proof_m=0}.  The predictions are accurate when $N$ is sufficiently large.

We can use the results of Theorems~\ref{thm:asymp_arg_complex_mean} and~\ref{thm:asymp_proof_m=0} to hypothesise about the behaviour of the estimators and also produce guidelines for selecting which estimator is better for a particular scenario.  A dominant factor in determining the variance of the angular least squares estimator is the term $(1 - f(-\nicefrac{1}{2}))^2$ that appears on the denominator of~(\ref{eq:asympvarm=0}).  The variance of the sample circular mean \eqref{eq:asympvarscm} does not depend so specifically on the value of the pdf at the antipodal point $f(-\nicefrac{1}{2})$. If the value of the pdf at $f(-\nicefrac{1}{2})$ is small then the angular least squares estimator tends to perform well. We refer to such distributions as `light-tailed'. Probably the extreme case of a `light-tailed' distribution is the wrapped uniform. In this case $f(-\nicefrac{1}{2}) = 0$ and the angular least squares estimator performs better than the sample circular mean (Figure~\ref{fig:directionest_Uniform}). The use of the wrapped uniform distribution may initially seem contrived, however, it is often used in practice for modeling the effect of quantisation noise \cite{Widrow-Kollar_quant_noise_2008}.  For example, in the problem of delay estimation \cite{Elton_circstat_radar_pulse_1994} the received signal is taken from a timer and the timer is typically quantised.  If the quantisation is very course, or alternatively the signal period is very small, then quantisation might be a dominant source of noise.  The angular least squares estimator would likely be a better choice in this scenario. 

Conversely if the pdf is `large' at $f(-\nicefrac{1}{2})$  (i.e. the distribution is `heavy-tailed') then we expect the variance of the angular least squares estimator to be (comparatively) large, and that the sample circular mean estimator would be better. This behaviour is probably somewhat seen with the von Mises distribution (Figure~\ref{fig:directionest_VonMises}), although not in a substantial manner. To test this hypothesis further we have run another set of simulations with the pdf
\begin{equation}\label{eq:uni+vm_dist}
f(\theta) = (1-p)v(\theta) + p
\end{equation}
where $0 < p < 1$ and where $v(\theta) = e^{\kappa\cos\left(2\pi \theta \right)}I_0^{-1}\left(p^{-1}\right)$ is the pdf of the zero mean von Mises distribution with \emph{concentration} $p^{-1}$ and $I_0(\cdot)$ is the zeroth order modified Bessel function \cite[Sec.~5.3]{McKilliam2010thesis}~\cite{Fisher1993,Mardia_directional_statistics}.  This distribution corresponds to a random variable that is von Mises with probability $1-p$ and otherwise is circular uniform (Figure~\ref{fig:circularuniformdist}) with probability $p$.  Figure~\ref{fig:pdf_sumunifvonmis} shows the pdf of this distribution when $p = 0.3$.  In practice a distribution of this type might occur when the observations contain outliers that can be modeled by the circular uniform distribution. %The distribution is unimodal and symmetric so the corresponding random variable has zero circular and unwrapped means by Theorem~\ref{thm:unimean}.  
The unwrapped variance increases monotonically with $p$.  An effect of including the circular uniform distribution is to increase the value of the pdf at the antipodal point given by $f(-\nicefrac{1}{2}) = p + (1-p)I_0^{-1}\left(p^{-1}\right)$.  So this distribution is particularly `heavy-tailed'. The performance of the estimators with this distribution is shown in Figure~\ref{fig:directionest_sumdist}. The sample circular mean convincingly outperforms the angular least squares estimator as expected.

\begin{figure}[tp]
	\centering
		\includegraphics[width=\linewidth]{figs/VonMisesSumUnif_0p3-1.mps}
		\caption{The pdf from~\eqref{eq:uni+vm_dist} when $p = 0.3$. The wrapped variance is approximately 0.03171 in this case.}
		\label{fig:pdf_sumunifvonmis}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{plots/directionestplot-1.mps}
		\caption{MSE versus unwrapped variance for the von Mises distribution.}
		\label{fig:directionest_VonMises}
\end{figure}
%\newpage

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{plots/directionestplot-2.mps}
		\caption{MSE versus unwrapped variance for the wrapped uniform distribution.}
		\label{fig:directionest_Uniform}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{plots/delayestplot-1.mps}
		\caption{MSE versus unwrapped variance for the wrapped normal distribution.}
		\label{fig:directionest_normal}
\end{figure}

\begin{figure}[p]
	\centering
		\includegraphics[width=\linewidth]{plots/sumdistplot-1.mps}
		\caption{MSE versus unwrapped variance for the distribution from~\eqref{eq:uni+vm_dist}.}
		\label{fig:directionest_sumdist}
\end{figure}

%\newpage


\section{Further remarks}

One topic that we have ignored is the practical computation of confidence intervals for the estimators.  If the distribution is known in advance then confidence intervals can be computed directly from the expressions for asymptotic variance given by Theorems~\ref{thm:asymp_arg_complex_mean} and~\ref{thm:asymp_proof_m=0}.  However, if the distributions is not known in advance, it is necessary to estimate confidence intervals from the data.  For the sample circular mean this is probably straightforward, an estimate of the circular variance and the second trigonometric moment, both required in (\ref{eq:asympvarscm}), can be made by taking the appropriate averages as explained in~\cite{Fisher_common_mean_direction_dir_est_no_dist_assumptions1983}. For the angular least squares estimator the problem is not so straightforward. The asymptotic variance~(\ref{eq:asympvarm=0}) depends on the unwrapped variance $\sigma^2$ and also the value of the pdf at the antipodal point $f(-\nicefrac{1}{2})$. An estimate of the unwrapped variance can be obtained by the average $N^{-1}\sum_{n=1}^{N}\fracpart{\Theta_n - \hat{\mu}}^2$ but the appropriate estimator of $f(-\nicefrac{1}{2})$ is not as obvious. Some type of density estimation is required \cite{Fisher1989smoothingcircdata,Rosenblatt_dens_est_1956,Parzen_dens_est_1962}, but we will not consider this further here. Given estimated confidence intervals it is potentially possible to compute both the sample circular mean and the angular least squares estimators and select the best estimator `on-line' using the confidence intervals.  This has the potential to produce a hybrid estimator that works well for both `heavy-tailed' and `light-tailed' distributions.

The angular least squares estimator has an obvious interpretation in more dimensions, that of minimising square arc-length on the sphere. However, the fast algorithm based on the lattice $A_n^*$ does not carry over into more dimensions. The modulo nature of the circle naturally imbues the combinatorial structure of a lattice. More precisely, the circle gives rise to a $\ints$-module, in this case an infinite discrete abelian group. However, in more dimensions, the unit-sphere does not correspond with a $\ints$-module, so there is no corresponding lattice. There may, however, exist other combinatorial structures for rapidly computing the analogue of the angular least squares estimator in more dimensions.

\section{Conclusion}

This paper has considered two notions of \term{mean direction}, the \term{circular mean} that is the most common definition and the \term{unwrapped mean} that is uncommon. We have considered methods for estimating these means. The first is the \term{sample circular mean} estimator of the circular mean that has received significant attention in the literature. The second estimator is the \term{angular least squares estimator} of the unwrapped mean. We showed how the angular least squares estimator can be rapidly computed by finding a nearest point in the lattice $A_n^*$. Theorem~\ref{thm:asymp_proof_m=0} showed that the angular least squares estimator is strongly consistent and asymptotically normally distributed. Section~\ref{sec:comparingestimatorcircmean} considered the performance of these estimators for a variety of circular distributions.  It was found that the angular least squares estimator tends to perform better when the distribution is `light-tailed' (when the antipodal point $f(-\nicefrac{1}{2})$ is small), whereas the sample circular mean tends to perform better when the distribution is `heavy-tailed' (when the antipodal point $f(-\nicefrac{1}{2})$ is large). We also found that the performance of the estimators is accurately modeled by the asymptotic results derived in~Theorems~\ref{thm:asymp_arg_complex_mean} and~\ref{thm:asymp_proof_m=0}.

\appendix

\begin{lemma}\label{lem:limsupSandES}
Let $L_N(\lambda)$ be given as in \eqref{eq:supELNLN}. Then 
\[
\sup_{\lambda \in [-\nicefrac{1}{2}, \nicefrac{1}{2})}| L_N(\lambda) - EL_N(\lambda) |
\] 
converges almost surely to zero as $N \rightarrow \infty$.
\end{lemma}
This type of result is common to a body of literature that, in recent times, has been called the \term{uniform law of large numbers}. An overview of some of these techniques is given by Amemiya \cite[Ch.~4]{amemiya1985advanced}.
\begin{proof}
Let $D_N = L_N - EL_N$ and $B = [-\nicefrac{1}{2},\nicefrac{1}{2})$ denote the unit interval. We want to show that $\sup_{b \in B} \vert D_N(b) \vert \rightarrow 0$ almost surely as $N\rightarrow \infty$. Partition $B$ into intervals
\[
B_k = \left[\frac{1}{2} + \frac{k-1}{N^{d}}, \frac{1}{2} + \frac{k}{N^{d} }\right) \,, \,\,\, k = 1,2,\dots,\floor{N^d} + 1
\]
for some $d > 0$ so that each interval $B_k$ has length $N^{d}$ and $B \subseteq \bigcup_{k}B_k$.  Let $b_k$ denote a \emph{fixed} point inside $B_k$. Now
\begin{align*}
\sup_{b \in B}\vert D_N(b) \vert &= \sup_{k}\sup_{b \in B_k}\vert D_N(b) \vert \\
&= \sup_{k}\sup_{b \in B_k}\vert D_N(b) - D_N(b_k) + D_N(b_k)\vert \\
&\leq \sup_{k}\vert D_N(b_k)\vert + \sup_{k}\sup_{b \in B_k}\vert D_N(b) - D_N(b_k)\vert.
\end{align*}
So, the proof is complete if we can show that $\sup_{k\in G}\vert D_N(b_k)\vert \rightarrow 0$ and $\sup_{k\in G}\sup_{b \in B_k}\vert D_N(b) - D_N(b_k)\vert \rightarrow 0$ almost surely as $N\rightarrow \infty$.  These are proved in Lemma~\ref{lem:DNij->0} and Lemma~\ref{lem:DNoverBij->0} which follow.
\end{proof}

\begin{lemma} \label{lem:DNij->0}
$\sup_{k\in G}\vert D_N(b_k)\vert \rightarrow 0$ almost surely as $N\rightarrow \infty$.
 \end{lemma}
\begin{proof}
Let $\beta$ be a positive integer.  For any fixed $b$ Markov's inequality gives
\[
 \prob\left(\left\vert D_N^{2\beta}(b) \right\vert > \epsilon^{2\beta}\right) \leq \frac{E\left[  \left\vert D_{N}^{2\beta}(b) \right\vert \right]  }{\varepsilon^{2\beta}}
\] 
for any $\varepsilon > 0$. As $\beta$ is a positive integer, $\left|D_N^{2\beta}(b)\right| = D_{N}^{2\beta}(b)$ and
\[
\prob\left(\left\vert D_N(b) \right\vert > \epsilon\right) \leq \frac{E \left[ D_N^{2\beta}(b) \right] }{\varepsilon^{2\beta}}.
\]
It is known (see, for example, Brillinger~\cite{Brillinger1962_moment_bounds_iid} or Lemma 9 from \cite{McKilliamFrequencyEstimationByPhaseUnwrapping2009}) that $E\left[ D_N^{2\beta}(b) \right] = O\left(N^{-\beta}\right)$.  Thus,
 \begin{align*}
 \prob\left(  \sup_{k}\abs{D_N(b_k)} >\varepsilon\right)  &\leq \sum_{k } \prob \left( \abs{D_N(b_k)} >\varepsilon\right) \\
 &\leq \sum_{k } \frac{E \left[ D_N^{2\beta}(b_k) \right] }{\varepsilon^{2\beta}} \\
&= (\floor{N^d} + 1)O\left(N^{-\beta}\right) = O\left(N^{d - \beta}\right).
\end{align*}
We can choose $\beta > d > 0$ so that the exponent $d - \beta$ is less than zero and this proves that $\sup_{k }\abs{D_N(b_k)}$ converges in \emph{probability} to zero as $N \rightarrow 0$. If we choose $\beta$ so that $d - \beta < -1$,
\[
 \sum_{N=1}^{\infty}\prob\left(  \sup_{k }\abs{D_N(b_k)} >\varepsilon\right) < \infty,
\]
and consequently $\sup_{k }\abs{ D_N(b_k)} \rightarrow 0$ almost surely as $N\rightarrow\infty$ by the Borel-Cantelli lemma.%~\citep[p.~46]{Billingsley1979_probability_and_measure}.
\end{proof}

\begin{lemma}\label{lem:DNoverBij->0}
$\sup_{k }\sup_{b \in B_k}\abs{D_N(b) - D_N(b_k)} \rightarrow 0$ almost surely as $N\rightarrow \infty$.
 \end{lemma}
\begin{proof}
The proof we give is not just \emph{almost surely} but \emph{surely} as the convergence will be shown to occur irrespective of the values of the $\Phi_1, \dots, \Phi_N$.  Let 
\[ 
A_n(b) = \fracpart{\Phi_n - b}^2 - \fracpart{\Phi_n - b_k}^2.
\]
For any $b$ in the interval $B_k$ we have $|b - b_k| \leq N^{-d}$ so, using Lemma~\ref{lem:boundedsquarefracparts} stated below,
\[
\abs{A_n(b)} =  \abs{\fracpart{\Phi_n - b}^2 - \fracpart{\Phi_n - b_k}^2} \leq N^{-d}.
\]
This bound is independent of the value of $\Phi_n$ so the same result is true of the expectation, that is, $E\abs{A_n(b)} \leq N^{-d}$.  Now
\begin{align*}
\abs{D_N(b) - D_N(b_k)} &= \abs{ \frac{1}{N}\sum_{n=1}^{N} \left( A_n(b) - EA_n(b) \right) } \\
&\leq \frac{1}{N}\sum_{n=1}^{N} \left( \abs{ A_n(b) } + E\abs{A_n(b)} \right)  \\ 
&\leq \frac{1}{N}\sum_{n=1}^{N} 2N^{-d} \leq 2N^{-d}
\end{align*}
As the bound is independent of $k$, we have $\sup_{k}\sup_{ b  \in B_k }\left\vert D_N(b) - D_N(b_k)\right\vert \leq 2N^{-d}$ and the proof follows.
\end{proof}

\begin{lemma}\label{lem:boundedsquarefracparts}
Let $x$ and $\delta$ be real numbers.  Then
\[
\fracpart{x}^2 - |\delta| \leq \fracpart{x + \delta}^2 \leq \fracpart{x}^2 + |\delta|.
\]
\end{lemma}
\begin{proof} (Sketch)
We omit a rigorous proof of this lemma which can be found in~\cite[Lemma~8.4]{McKilliam2010thesis}. The lemma is `obvious' because the squared fractional part function $\fracpart{x}^2$ is continuous and piecewise differentiable and the derivative has magnitude less than one whenever it exists.
\end{proof}

\begin{lemma}\label{lem:EI_n}
\[
\frac{1}{\sqrt{N}}\sum_{n=1}^{N} \round{ \Phi_n - \hat{\lambda}} = o_P(1) - \sqrt{N}\hat{\lambda}\big( f(-\nicefrac{1}{2}) + o_P(1) \big)
\]
where $o_P(1)$ converges in probability to zero as $N\rightarrow\infty$.
\end{lemma}
\begin{proof}
Let $k_n(\lambda) = \round{\Phi_n - \lambda}$ and let $F(x)$ be the cumulative distribution function of $\Phi_n$.  Because $f(\fracpart{x})$ is continuous at $-\nicefrac{1}{2}$ then $F(x)$ is continuous and differentiable on $[-\nicefrac{1}{2}, -\nicefrac{1}{2} + \delta] \cup [\nicefrac{1}{2} - \delta, \nicefrac{1}{2}]$ for some $\delta > 0$.  Now
\begin{align*}
E k_n(\lambda) &= \begin{cases}
-\int_{-\nicefrac{1}{2}}^{-\nicefrac{1}{2} + \lambda}f(x)\,dx, &  \lambda > 0 \\
\int_{\nicefrac{1}{2} + \lambda}^{\nicefrac{1}{2}}f(x)\,dx, &  \lambda < 0
\end{cases} \\
&= \begin{cases}
-F(-\nicefrac{1}{2} + \lambda), &  \lambda > 0 \\
1 - F(\nicefrac{1}{2} + \lambda), &  \lambda < 0.
\end{cases} 
\end{align*}
Because $f(\fracpart{x})$ is continuous at $-\nicefrac{1}{2}$ then by the mean value theorem
\[
E k_n(\lambda) = -\lambda \big( f(-\nicefrac{1}{2}) + o(1) \big)
\] 
as $\lambda \rightarrow 0$.  Now, results from the asymptotic theory of empirical distribution functions~\cite{Billingsley1999_convergence_of_probability_measures} show that, if $\lambda > 0$, then $N^{-1/2} \sum_{n=1}^{N} \big( k_n(\lambda) - Ek_n(\lambda) \big)$ converges in distribution to $W(\lambda)$, a Gaussian element in the space $D[0,1]$ of functions that are right continuous and have left-hand limits, with zero mean and covariance
\begin{align*}
\operatorname{cov}&( W(\lambda_1), W(\lambda_2) ) \\
&= 1 - F( \max(\lambda_1, \lambda_2) ) - \big( 1 - F(\lambda_1)  \big) \big( 1 - F(\lambda_2)  \big).
\end{align*}

Similarly, if $\lambda < 0$, then $N^{-1/2} \sum_{n=1}^{N} \big( k_n(\lambda) - Ek_n(\lambda) \big)$ converges in distribution to $W(\lambda) \in D[0,1]$ with zero mean and covariance
\begin{align*}
\operatorname{cov}&( W(\lambda_1), W(\lambda_2) ) \\
&= F( \max(\lambda_1, \lambda_2) ) - F(\lambda_1) F(\lambda_2).
\end{align*}
Consequently, since in either case $W(\lambda)$ converges in probability to zero as $\lambda$ converges to zero,
\[
\sup_{\abs{\lambda}} \frac{1}{\sqrt{N}}\abs{ \sum_{n=1}^{N} \big( k_n(\lambda) - Ek_n(\lambda) \big)  } \rightarrow 0
\]
in probability as $\delta \rightarrow 0$. Thus, since $\hat{\lambda} \rightarrow 0$ almost surely as $N\rightarrow\infty$,
\[
\frac{1}{\sqrt{N}}\sum_{n=1}^{N} k_n(\hat{\lambda}) = W(\hat{\lambda}) + o_P(1) - \sqrt{N}\hat{\lambda}\big( f(-\nicefrac{1}{2}) + o_P(1) \big).
\]
The proof follows since $W(\hat{\lambda})$ converges in probability to zero as $N\rightarrow\infty$.
\end{proof}


% %%%This is my version of the proof which uses the Glivenko-Cantelli lemma and I think works fine too.
% \begin{lemma} (Glivenko-Cantelli version)
% \[
% \frac{1}{\sqrt{N}}\sum_{n=1}^{N} \round{ \Phi_n - \hat{\lambda}} = -\sqrt{N}\hat{\lambda}\big( f(-\nicefrac{1}{2}) + o(1) \big)
% \]
% where $o(1)$ converges almost surely to zero as $N\rightarrow\infty$.
% \end{lemma}
% \begin{proof}
% Let $F(x)$ be the cumulative distribution function of $\Phi_1$. Because $f(\fracpart{x})$ is continuous at $-\nicefrac{1}{2}$ then $F(x)$ is continuous and differentiable on $[-\nicefrac{1}{2}, -\nicefrac{1}{2} + \delta] \cup [\nicefrac{1}{2} - \delta, \nicefrac{1}{2}]$ for some $\delta > 0$. Let 
% \[
% G_N(\lambda) = \frac{1}{N}\sum_{n=1}^{N} \round{ \Phi_n - \lambda}
% \]
% If $\lambda \geq 0$
% \[
% G_N(\lambda) = -\frac{1}{N}\sum_{n=1}^{N} I( \Phi_n < -\nicefrac{1}{2} + \lambda)
% \]
% where $I(\cdot)$ is an indicator function that is one when its argument is true and zero otherwise. By the Glivenko-Cantelli lemma
% \begin{equation}\label{eq:suplambda>0}
% \sup_{\lambda \in [\nicefrac{1}{2} - \delta, \nicefrac{1}{2}]}\abs{ F(-\nicefrac{1}{2} + \lambda) + G_N(\lambda) } \rightarrow 0
% \end{equation}
% almost surely as $N\rightarrow \infty$.  Similarly when $\lambda < 0$
% \[
% G_N(\lambda) = \frac{1}{N}\sum_{n=1}^{N} I( \Phi_n > \nicefrac{1}{2} + \lambda)
% \]
% and
% \begin{equation}\label{eq:suplambda<0}
% \sup_{\lambda \in [-\nicefrac{1}{2}, -\nicefrac{1}{2} + \delta]}\abs{ 1 - F(\nicefrac{1}{2} + \lambda) + G_N(\lambda) } \rightarrow 0.
% \end{equation}

% Let
% \[
% f_\delta = \sup_{\abs{x} < \delta}\abs{ f(-\nicefrac{1}{2}) - f(\fracpart{-\nicefrac{1}{2} + x})}
% \]
% and note that $f_\delta$ can be made abitrarily small by choosing $\delta$ small. When $\lambda \geq 0$
% \[
% F(-\nicefrac{1}{2} + \lambda) = \int_{-\nicefrac{1}{2}}^{-\nicefrac{1}{2} + \lambda}f(x) dx
% \]
% and for all $\delta > \lambda \geq 0$
% \[
% \lambda( f(-\nicefrac{1}{2}) - f_\delta) \leq F(-\nicefrac{1}{2} + \lambda) \leq \lambda( f(-\nicefrac{1}{2}) + f_\delta) .
% \]
% Using  (\ref{eq:suplambda>0}) gives
% \begin{equation} \label{eq:Gineq}
% \lambda( f(-\nicefrac{1}{2}) - f_\delta) \leq -G_N(\lambda) \leq \lambda( f(-\nicefrac{1}{2}) + f_\delta) .
% \end{equation}

% Similarly for all $\lambda < 0$
% \[
% 1 - F(\nicefrac{1}{2}) = \int_{\nicefrac{1}{2} + \lambda}^{\nicefrac{1}{2}}f(x) dx
% \]
% and for all $-\delta < \lambda < 0$
% \[
% \lambda( f(-\nicefrac{1}{2}) - f_\delta) \geq F(-\nicefrac{1}{2} + \lambda) - 1 \geq \lambda( f(-\nicefrac{1}{2}) + f_\delta ).
% \] 
% Using (\ref{eq:suplambda<0}) gives \eqref{eq:Gineq} again. So the inequality \eqref{eq:Gineq} holds for all $\lambda$ such that $-\delta < \lambda < \delta$. In view of strong consistency $\abs{\hat{\lambda}} < \delta$ with probability 1 as $N\rightarrow\infty$. So, with probability 1,
% \[
% \hat{\lambda}( f(-\nicefrac{1}{2}) - f_\delta) \geq -G_N(\hat{\lambda}) \geq \hat{\lambda}( f(-\nicefrac{1}{2}) + f_\delta ).
% \]
% as $N\rightarrow\infty$. The proof follows by multiplying this inequality by $\sqrt{N}$ and noting that $f_\delta$ can be chosen arbitrarily small.
% \end{proof}


\small
\bibliography{bib}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
